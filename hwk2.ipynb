{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 4700 - Homework #2\n",
    "#### Due Date: Wednesday, 11/14 @ 6pm on CMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 1: Introduction\n",
    "In this assignment, you will be implementing a decision tree learner in Python 3. Before we can start, let us import some functions that we will need. Please ensure the file *utils.py* is in the same directory as your Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from math import log\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the decision tree learner will take in, as input, a dataset containing examples. These examples each contain a sequence of attributes. Given some target attribute, the learner will create a decision tree that will (hopefully) be shallow and, for each example in the dataset, output the correct value for the target attribute. Before we begin to code the learner, we will need to first define what exactly is a decision tree. First, let's begin by defining the leaf node of a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A leaf node of a decision tree which holds just a result.\n",
    "\"\"\"\n",
    "class DTLeaf:\n",
    "    def __init__(self, result):\n",
    "        self.result = result\n",
    "\n",
    "    def __call__(self, example):\n",
    "        return self.result\n",
    "\n",
    "    def display(self, indent = 0):\n",
    "        print('Result =', self.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we define the internal (meaning non-leaf) node of the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "An internal node of a decision tree which tests an attribute, along with a\n",
    "dictionary of branches, one for each of the attribute's values.\n",
    "\"\"\"\n",
    "class DTInternal:\n",
    "    \"\"\"\n",
    "    Initialize by saying what attribute this node tests.\n",
    "    \"\"\"\n",
    "    def __init__(self, attr, attrName = None, defaultChild = None,\n",
    "                 branches = None):\n",
    "        self.attr = attr\n",
    "        self.attrName = attrName or attr\n",
    "        self.defaultChild = defaultChild\n",
    "        self.branches = branches or {}\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Given an example, classify it using the attribute and the branches.\n",
    "    \"\"\"\n",
    "    def __call__(self, example):\n",
    "        attrVal = example[self.attr]\n",
    "        if attrVal in self.branches:\n",
    "            return self.branches[attrVal](example)\n",
    "        else:\n",
    "            return self.defaultChild(example)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Add a branch.\n",
    "    \"\"\"\n",
    "    def add(self, val, subtree):\n",
    "        self.branches[val] = subtree\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Prints out the decision tree starting at this internal node using recursion.\n",
    "    \"\"\"\n",
    "    def display(self, indent = 1):\n",
    "        print ('Testing attribute:', self.attrName)\n",
    "        for (val, subtree) in self.branches.items():\n",
    "            print(' ' * 4 * indent, self.attrName, '=', val, '==>', end = ' ')\n",
    "            subtree.display(indent + 1)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are 4 functions that will prove to be useful in your implementation of the decision tree learner. Take some time to go over what each function is doing and familiarize yourself with how to use them. This will make the assignment easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multiply each number by a constant such that the sum is 1.0\n",
    "\"\"\"\n",
    "def normalize(dist):\n",
    "    if isinstance(dist, dict):\n",
    "        total = sum(dist.values())\n",
    "        for key in dist:\n",
    "            dist[key] = dist[key] / total\n",
    "            assert 0 <= dist[key] <= 1, 'Probabilities must be between 0 and 1.'\n",
    "        return dist\n",
    "    total = sum(dist)\n",
    "    return [(n / total) for n in dist]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Return a copy of the input with all occurrences of item removed.\n",
    "\"\"\"\n",
    "def removeAll(item, seq):\n",
    "    if isinstance(seq, str):\n",
    "        return seq.replace(item, '')\n",
    "    else:\n",
    "        return [x for x in seq if x != item]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Randomly shuffle a copy of the input.\n",
    "\"\"\"\n",
    "def shuffled(seq):\n",
    "    items = list(seq)\n",
    "    random.shuffle(items)\n",
    "    return items\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Return an element with the highest value according to key, which is a function.\n",
    "We break ties by first shuffling the input.\n",
    "\"\"\"\n",
    "def argmaxRandomTie(seq, key):\n",
    "    return max(shuffled(seq), key = key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a decision tree now defined, we can begin to code the learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 2: The First Half of the Learner\n",
    "We will be modeling our code closely to the pseudo-code from the textbook. Your first task is to implement the function *__pluralityVal__*. Recall that, given examples, we define the plurality value as the value of the target attribute that appears most often. As a hint, you will likely need to use both *__count__* (defined below) as well as *__argmaxRandomTie__*. Note, to make *__pluralityVal__* work easier with the learner, it should return a leaf node containing the plurality value, rather than just the value itself. We have also provided a small test to check if your function is returning the correct value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Count the number of examples that have example[attr] = val.\n",
    "\"\"\"\n",
    "def count(attr, val, examples):\n",
    "    return sum(e[attr] == val for e in examples)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Purpose: A default value needs to be returned when there are no examples for a combination of attribute values. This is achieved\n",
    "         by returning the Plurality Value of all the examples that were used to construct the node's parent.\n",
    "Inputs: examples = te , values = tv , target = tt\n",
    "Outputs: Leaf Node of Decision Tree which holds the element with the highest value according to the function key(lambda: key)\n",
    "Logic: We first compute the function that needs to be passed as key to the argmaxRandomTie. This is done by using the count\n",
    "       function and passing tv[tt] to retrieve the count for each element in tv[tt] i.e [0,1,2]. For each of these values, the\n",
    "       argmaxRandomTie determines the count values and returns the maximum(count(tv[tt])). This is then created as an object of\n",
    "       DTLeaf and returned.\n",
    "\"\"\"\n",
    "def pluralityVal(examples, values, target):\n",
    "    \n",
    "    newLeaf = argmaxRandomTie(values[target],lambda key: count(target,values,examples))\n",
    "    return DTLeaf(newLeaf)\n",
    "    \n",
    "\n",
    "\n",
    "# Test for pluralityVal\n",
    "te = [[3,2,1], [1,0,1], [4,3,2], [3,0,0], [5,2,1]]\n",
    "tv = [[1,2,3,4,5], [0,1,2,3], [0,1,2]]\n",
    "tt = 2\n",
    "assert pluralityVal(te, tv, tt).result == 1, 'Failed pluralityVal test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to determine if all the examples have the same classification. We will do this in the function *__allSameClass__*, which should simply return a Boolean value. Once again, a couple of tests have been added to ensure you are returning the correct output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Purpose: We need a method to check if the the examples are split according to their Goal States so that we need not\n",
    "         explore that branch further. allSameClass determines if the all examples at the child after splitting the parent on some \n",
    "         attributes fall in either of the two goal states.\n",
    "Inputs: examples = te , target = tt\n",
    "Outputs: Return False if te[tt] is not same for all the elements in te. Else return True\n",
    "Logic: We take the value in the first element of te at index tt and assign it to indvalue. Later loop through all the elements\n",
    "       present in te comparing the te[tt] value to indvalue to determine the output.\n",
    "Tricks: Take the value at index [target] in examples and compare it to the rest of the examples at the same index\n",
    "\"\"\"\n",
    "def allSameClass(examples, target):\n",
    "    indvalue = examples[0][target]\n",
    "    \n",
    "    for example in examples:\n",
    "        if(example[target] != indvalue):\n",
    "            return False\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Tests for allSameClass\n",
    "te = [[3,2,1], [1,0,1], [4,3,2], [3,0,0], [5,2,1]]\n",
    "tt = 1\n",
    "assert not allSameClass(te, tt), 'Failed allSameClass test #1'\n",
    "te = [[3,0,1], [1,0,1], [4,0,2], [3,0,0], [5,0,1]]\n",
    "assert allSameClass(te, tt), 'Failed allSameClass test #2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have completed enough functions for the first half of the decision tree learner. We will now focus on the second half."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 3: The Second Half of the Learner\n",
    "We need to some way to determine the most important attribute in our dataset that we should split next on for our tree. There are many ways for us to define \"importance.\" One common way to do so is by looking at the information gain that an attribute provides. This gain is defined in terms of the entropy. Recall that entropy can be calculated as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H(V) &= - \\sum_{k} P(v_k)\\ log_2 P(v_k)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that the above definition relies on probabilities. Can you assume that the attribute values being passed in are probabilities? Consider the answer to this in your implementation of this equation in the function *__entropy__*. If correct, you should be able to pass the 2 tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Purpose: Entropy is required to calculate the uncertainty of a random variable. Here this random variable are the set of values \n",
    "         which it can take. In Binary case,(B) the set of values is either true(1) or false(0). But in Multi Value case(H), this\n",
    "         set can contain any admissible value.\n",
    "Inputs: values = tv\n",
    "Outputs: entropy of the values which is calculated using the formula −∑kP(vk)log2P(vk)\n",
    "Logic: We first calculate the sum of the values. Iterate over each value in values and divide it by the sum to get the probability \n",
    "       of that value and append it into an array. Now we have a list of probabilities for each value in values. With this, we check\n",
    "       if that element is 0 since log(0) is undefined and use the formula to calculate the entropy.\n",
    "Tricks: Each element of probability is converted to float for better accuracy\n",
    "\"\"\"\n",
    "def entropy(values):\n",
    "    probarray = []\n",
    "    entropy = []\n",
    "    summ = sum(values)\n",
    "    if summ == 0:\n",
    "        return 0\n",
    "    for i in values:\n",
    "        probarray.append(i/summ)\n",
    "    \n",
    "    for i in probarray:\n",
    "        if i != 0:\n",
    "            ifloat = float(i)\n",
    "            entropy.append(ifloat*math.log2(ifloat))\n",
    "        \n",
    "    return(-1*sum(entropy))\n",
    "    \n",
    "\n",
    "# Tests for entropy\n",
    "tv = [13, 5, 2, 20, 4, 10, 4]\n",
    "assert abs(entropy(tv) - 2.4549947941466774) <= 1e-3, 'Failed entropy test #1'\n",
    "tv = [0, 0, 5, 0, 0, 0, 0]\n",
    "assert entropy(tv) == 0.0, 'Failed entropy test #2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the entropy now being calculated, we are able to determine the information gain of an attribute. The calculation for this is as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "B(q) &= -[q\\ log_2 q + (1 - q)\\ log_2 (1 - q)] \\\\\n",
    "Remainder(A) &= \\sum_{k = 1}^{d} \\frac{p_k + n_k}{p + n}\\ B(\\frac{p_k}{p_k + n_k}) \\\\\n",
    "InfoGain(A) &= B(\\frac{p}{p + n}) - Remainder(A)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "As a reminder, \\\\(q\\\\) is the probability of a Boolean random variable being true, \\\\(p\\\\) is the number of positive examples in the training set, \\\\(n\\\\) is the number of negative examples, while their subscripted versions are analogous but for the different values of attribute \\\\(A\\\\). You will need to implement the above calculations in the function *__infoGain__*. You might find it useful to use *__splitBy__* (defined below), since calculating \\\\(Remainder\\\\) requires us to split an attribute \\\\(A\\\\) by its \\\\(d\\\\) distinct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Return a list of (value, examples) pairs for each val of attr.\n",
    "\"\"\"\n",
    "def splitBy(attr, examples, values):\n",
    "    return [(v, [e for e in examples if e[attr] == v]) for v in values[attr]]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Purpose: InfoGain is used to choose the attribute which provides the maximum gain. In other words, it provides a formal measure\n",
    "         to evaluate the attributes such that the children produced after the split would most likely fall in same Goal States.\n",
    "        The children may still contain a mixed set of examples which is then split again \n",
    "         on some attribute by using InfoGain. The attribute that gives the maximum Gain is the attribute on which the tree is \n",
    "         split. The Remainder function yields the expected etnropy remaining after testing the attribute evaluated. \n",
    "Inputs: attributes = ta , examples = te , values = tv , target = tt\n",
    "Outputs: The InfoGain from the attribute test on attr to yield the expected reduction in entropy.\n",
    "Logic: 1)We first use the count function to retrieve the number of times each value is repeated at target location in examples.\n",
    "       Since the numbers at the target location in examples is not Binary(0,1,2), we use the entropy function from above to \n",
    "       calculate the uncertainity of these values. Next we use the splitBy function to retrieve the value and the examples that\n",
    "       contain the value at the index attribute in examples. This gives us:\n",
    "       x = [(1, [[1, 0, 1]]), (2, []), (3, [[3, 2, 1], [3, 0, 0]]), (4, [[4, 3, 2]]), (5, [[5, 2, 1]])]. In other words, we search\n",
    "       through examples for each value where examples[attribute] = value and map them. Hence 1 is mapped to 1,0,1 since the first\n",
    "       index has the value 1. No examples start with 2. 3,2,1 and 3,0,0 start with 3 and hence mapped to 3 and so on.\n",
    "       2) Next we calculate the sum of the lengths of the second parameter which gives us the total example size(p+n).\n",
    "       3) Next for each term in x we need to decide the positive and negative states for that example. For this, we use the count\n",
    "       and the entropy function from above. ∑(pk+nk)/(p+n)\n",
    "       4) Now we calculate each term of the remainder by multiplying two terms,First Term and Second Term. The First Term is \n",
    "       nothing but the length of the length of the second column in x divided by the term calculated in 2)(H(p/p+n)). \n",
    "       The Second Term is entropy as calculated by 3). All these terms are added up to give the remainder.\n",
    "       5)Subtract 5) from 1) to get the info gain on the attribute.  \n",
    "\"\"\"\n",
    "def infoGain(attr, examples, values, target):\n",
    "    cnt = []\n",
    "    for val in values[target]:\n",
    "        cnt.append(count(target,val,examples)) \n",
    "    \n",
    "    newentropy = entropy(cnt) #B(p/(p+n))\n",
    "    \n",
    "    x = splitBy(attr,examples,values)\n",
    "    totallength = 0\n",
    "    for i,j in x:\n",
    "        totallength += len(j)\n",
    "    \n",
    "    innercnt = []\n",
    "    summultiply = 0\n",
    "    for i,j in x:\n",
    "        firstterm = len(j)/totallength\n",
    "\n",
    "        for val in values[target]:\n",
    "            innercnt.append(count(target,val,j))\n",
    "        \n",
    "        \n",
    "        secondentropy = entropy(innercnt)\n",
    "        summultiply += firstterm*secondentropy\n",
    "        innercnt = []\n",
    "        \n",
    "    return (newentropy - summultiply)\n",
    "    \n",
    "\n",
    "# Tests for infoGain\n",
    "te = [[3,2,1], [1,0,1], [4,3,2], [3,0,0], [5,2,1]]\n",
    "tv = [[1,2,3,4,5], [0,1,2,3], [0,1,2]]\n",
    "tt = 2\n",
    "ta = 0\n",
    "assert abs(infoGain(ta, te, tv, tt) - 0.9709505944546687) <= 1e-3, \\\n",
    "       'Failed infoGain test #1'\n",
    "tt = 1\n",
    "assert abs(infoGain(ta, te, tv, tt) - 1.121928094887362) <= 1e-3, \\\n",
    "       'Failed infoGain test #2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last bit of functionality that our learner needs is to determine which attribute we should choose to split on. To recap, the decision tree learner greedily chooses the attribute that provides the most information gain as the splitting point. Capture this logic in the function *__chooseAttr__*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Purpose: Now that we know what InfoGain is used for, each attribute is evaluated for its InfoGain and the attribute that gives\n",
    "         maximum InfoGain is used to split the decision tree.\n",
    "Inputs: attrs = ta , examples = te , values = tv , target = tt\n",
    "Outputs: The attribute that will be used to split the decision tree based on the maximum InfoGain.\n",
    "Logic: For each of the attributes provided in attrs, we evaluate the InfoGain and find the attribute that yields the maximum \n",
    "       InfoGain and return that attribute.\n",
    "Tricks: Start with a maximum as negative infinity and then keep assigning the maximum as and when encountered\n",
    "\"\"\"\n",
    "def chooseAttr(attrs, examples, values, target):\n",
    "    maxi = -math.inf\n",
    "    predictedattr = 0\n",
    "    for val in attrs:\n",
    "        x = infoGain(val, examples, values, target)\n",
    "        if(x > maxi):\n",
    "            maxi = x\n",
    "            predictedattr = val\n",
    "\n",
    "    return predictedattr\n",
    "\n",
    "\n",
    "# Test for chooseAttr\n",
    "te = [[6,7,8,7,0], [7,3,4,0,0], [1,0,2,3,3], [8,5,9,1,1], [9,1,5,4,2],\n",
    "      [9,5,6,8,3], [4,6,0,1,6], [8,5,4,4,2], [0,3,7,2,6], [7,9,3,1,2]]\n",
    "tv = [[0,1,2,3,4,5,6,7,8,9] for i in range(10)]\n",
    "tt = 3\n",
    "ta = [0,1,2,4]\n",
    "assert chooseAttr(ta, te, tv, tt) == 2, 'Failed chooseAttr test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the necessary functions to implement our learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 4: Putting It All Together\n",
    "With everything now in place, we should now be able to implement the decision tree learner in full. For easier reference, the pseudo-code from the textbook (fig. 18.5) has been reproduced below. Use this to help guide your implementation in the function *__DTLearner__*.\n",
    "\n",
    "**function** DECISION-TREE-LEARNING(*examples*, *attributes*, *parent_examples*) **returns** a tree<br/>\n",
    "&nbsp;&nbsp;**if** *examples* is empty **then return** PLURALITY-VALUE(*parent_examples*)<br/>\n",
    "&nbsp;&nbsp;**else if** all *examples* have the same classification **then return** the classification<br/>\n",
    "&nbsp;&nbsp;**else if** *attributes* is empty **then return** PLURALITY-VALUE(*examples*)<br/>\n",
    "&nbsp;&nbsp;**else**<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\\\\(A \\leftarrow argmax_{a \\in attributes}\\\\) IMPORTANCE(\\\\(a\\\\), *examples*)<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;*tree* \\\\(\\leftarrow\\\\) a new decision tree with root test \\\\(A\\\\)<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**for each** value \\\\(v_k\\\\) of \\\\(A\\\\) **do**<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*exs* \\\\(\\leftarrow\\\\) {\\\\(e : e \\in\\\\) *examples* **and** *e.A* = \\\\(v_k\\\\)}<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*subtree* \\\\(\\leftarrow\\\\) DECISION-TREE-LEARNING(*exs*, *attributes* − \\\\(A\\\\), *examples*)<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;add a branch to *tree* with label (\\\\(A\\\\) = \\\\(v_k\\\\)) and subtree *subtree*<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**return** *tree*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing attribute: 7\n",
      "     7 = 0 ==> Result = 1\n",
      "     7 = 1 ==> Result = 0\n",
      "     7 = 2 ==> Testing attribute: 11\n",
      "         11 = 0 ==> Result = 0\n",
      "         11 = 1 ==> Result = 2\n",
      "         11 = 2 ==> Result = 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Purpose: With all the necessary functions defined, we design a Decision Tree Learner. This DTLearner knows which tree to expand\n",
    "         based on the attributes(inturn based on InfoGain) such that the leaf nodes contain same Goal States.\n",
    "Inputs: examples = te, attrs = ta, values = tv , target = tt, attrNames = ta , parentExamples = default value calculated \n",
    "        from the plurality classification of all the examples that were used in constructing the node’s parent \n",
    "Outputs: A Decision Tree Learner\n",
    "Logic: 1) if the examples are empty meaning that they are already properly evaluated into corresponding leaf nodes such that\n",
    "          each leaf node contains examples of same Goal State then return the Plurality of their parents.\n",
    "       2) if all of the examples are proved to be in the same Goal State, then we just return that Leaf\n",
    "       3) if the attributes are empty meaning that there is no proper attribute given or to evaluate on, then return the \n",
    "          Plurality of the examples.\n",
    "       4) if none of 1,2,3 hold then we first choose the attribute based on maximum InfoGain to split the decision tree.\n",
    "          Next we create an internal node of a decision tree which tests an attribute, along with a dictionary of branches, \n",
    "          one for each of the attribute's values by using DTInternal. Next we split the DecisionTree based on the attribute chosen\n",
    "          and create a dictionary of the children. Remove the attribute from the list since it was already evaluated. The Children\n",
    "          are again subject to the DTLearner to evaluate which attribute and branch need to be expanded further. This recursion\n",
    "          is performed until all the examples reach their respective Goal States.\n",
    "          \n",
    "\"\"\"\n",
    "def DTLearner(examples, attrs, values, target, attrNames, parentExamples = ()):\n",
    "    if not examples:\n",
    "        return pluraityVal(parentExamples,values,target)\n",
    "    elif allSameClass(examples, target):\n",
    "        return DTLeaf(examples[0][target])\n",
    "    elif not attrs:\n",
    "        return pluraityVal(examples,values,target)\n",
    "    else:\n",
    "        A = chooseAttr(attrs, examples, values, target)\n",
    "        tree = DTInternal(A)\n",
    "        splitby = splitBy(A,examples,values)\n",
    "        diction = dict(splitby)\n",
    "        attrs.remove(A)\n",
    "        for x in values[A]:\n",
    "            ex = DTLearner(diction[x] , attrs,values,target,attrNames,examples)\n",
    "            tree.add(x,ex)\n",
    "        return tree\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Test for DTLearner\n",
    "\n",
    "Output should match:\n",
    "\n",
    "Testing attribute: 7\n",
    "     7 = 0 ==> Result = 1\n",
    "     7 = 1 ==> Result = 0\n",
    "     7 = 2 ==> Testing attribute: 11\n",
    "         11 = 0 ==> Result = 0\n",
    "         11 = 1 ==> Result = 2\n",
    "         11 = 2 ==> Result = 1\n",
    "\"\"\"\n",
    "te = [[1,1,1,1,1,1,2,2,2,0,2,0,0,1,2,0,0,2,2,0],\n",
    "      [2,0,1,2,1,1,1,2,2,2,2,1,0,2,2,0,2,2,2,2],\n",
    "      [1,2,1,1,1,2,2,2,0,1,2,2,2,2,2,1,2,0,2,1],\n",
    "      [2,2,2,1,0,1,2,2,2,0,2,1,0,1,1,1,0,2,0,2],\n",
    "      [0,1,1,0,0,0,2,0,1,0,1,2,2,2,2,0,0,0,1,1],\n",
    "      [0,2,0,0,0,1,0,1,0,2,1,1,2,0,2,2,0,2,0,0],\n",
    "      [0,0,1,2,0,0,0,0,1,2,0,0,2,0,0,0,0,2,1,1],\n",
    "      [0,2,2,1,1,0,0,2,2,0,2,1,1,0,0,2,0,2,1,2],\n",
    "      [1,0,2,0,1,2,2,1,0,1,0,2,1,2,0,0,1,1,2,0],\n",
    "      [0,2,0,1,2,1,1,1,1,0,1,2,2,0,1,2,1,0,0,0]]\n",
    "tv = [[j for j in range(3)] for i in range(20)]\n",
    "tt = 19\n",
    "ta = [i for i in range(19)]\n",
    "tn = ta\n",
    "DTLearner(te, ta, tv, tt, tn).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all the above is working, we should now be able to train, display, and test our decision tree! The code block below will load in the restaurant data from the textbook, use it as training data for our learner, output the learned decision tree, and then use that to try to classify a test point. To help with this process, we use the *__DTLCaller__* to extract the relevant information from our dataset and pass that into the learner. Please ensure you have the *restaurant.csv* in the same directory as your Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing attribute: 4\n",
      "     4 = None ==> Result = No\n",
      "     4 = Full ==> Testing attribute: 8\n",
      "         8 = Thai ==> Testing attribute: 2\n",
      "             2 = Yes ==> Result = Yes\n",
      "             2 = No ==> Result = No\n",
      "\n",
      "         8 = Burger ==> Testing attribute: 0\n",
      "             0 = Yes ==> Result = Yes\n",
      "             0 = No ==> Result = No\n",
      "\n",
      "         8 = French ==> Result = Yes\n",
      "         8 = Italian ==> Result = No\n",
      "\n",
      "     4 = Some ==> Result = Yes\n",
      "\n",
      "No\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Front-end caller for the decision tree learner. Given an input dataset, it will\n",
    "extract the necessary information and pass it in to the learner.\n",
    "\"\"\"\n",
    "def DTLCaller(dataset):\n",
    "    return DTLearner(dataset.examples, dataset.inputs, dataset.values,\n",
    "                     dataset.target, dataset.attrNames)\n",
    "\n",
    "\n",
    "# Load the restaurant dataset from a CSV file\n",
    "restaurant = Dataset(name = 'restaurant', target = 'Wait',\n",
    "                     attrNames = 'Alternate Bar Fri/Sat Hungry Patrons Price ' +\n",
    "                     'Raining Reservation Type WaitEstimate Wait')\n",
    "\n",
    "\n",
    "# Feed the dataset into the decision tree learner\n",
    "dtlRestaurant = DTLCaller(restaurant)\n",
    "\n",
    "\n",
    "# Display the decision tree that is learned\n",
    "dtlRestaurant.display()\n",
    "\n",
    "\n",
    "# Try classifying new test data\n",
    "print(dtlRestaurant(['Yes','No','No','Yes','Full',\n",
    "                     '$$','No','No','Italian','0-10']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use additional datasets. For example, the code block below uses a zoo dataset to train our learner and find a decision tree for our data. The zoo dataset contains a list of animals, each of which has different features about the animal, along with what kind of animal it is. We use our learned decision tree to determine what kind of animal a dragonfly is. Please ensure you have the *zoo.csv* in the same directory as your Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing attribute: 13\n",
      "     13 = 0 ==> Testing attribute: 12\n",
      "         12 = 0 ==> Testing attribute: 8\n",
      "             8 = 0 ==> Result = shellfish\n",
      "             8 = 1 ==> Result = reptile\n",
      "\n",
      "         12 = 1 ==> Testing attribute: 3\n",
      "             3 = 0 ==> Result = mammal\n",
      "             3 = 1 ==> Result = fish\n",
      "\n",
      "\n",
      "     13 = 2 ==> Testing attribute: 1\n",
      "         1 = 0 ==> Result = bird\n",
      "         1 = 1 ==> Result = mammal\n",
      "\n",
      "     13 = 4 ==> Testing attribute: 4\n",
      "         4 = 0 ==> Testing attribute: 6\n",
      "             6 = 0 ==> Result = reptile\n",
      "             6 = 1 ==> Testing attribute: 9\n",
      "                 9 = 0 ==> Result = shellfish\n",
      "                 9 = 1 ==> Result = amphibian\n",
      "\n",
      "\n",
      "         4 = 1 ==> Result = mammal\n",
      "\n",
      "     13 = 5 ==> Result = shellfish\n",
      "     13 = 6 ==> Testing attribute: 10\n",
      "         10 = 0 ==> Result = shellfish\n",
      "         10 = 1 ==> Result = insect\n",
      "\n",
      "     13 = 8 ==> Result = shellfish\n",
      "\n",
      "insect\n"
     ]
    }
   ],
   "source": [
    "# Load the zoo dataset from a CSV file\n",
    "zoo = Dataset(name = 'zoo', target = 'type', exclude = ['name'],\n",
    "              attrNames = 'name hair feathers eggs milk airborne aquatic ' +\n",
    "              'predator toothed backbone breathes venomous fins legs tail ' +\n",
    "              'domestic catsize type')\n",
    "\n",
    "\n",
    "# Feed the zoo dataset into the decision tree learner\n",
    "dtlZoo = DTLCaller(zoo)\n",
    "\n",
    "\n",
    "# Display the decision tree that is learned\n",
    "dtlZoo.display()\n",
    "\n",
    "\n",
    "# Try classifying new test data\n",
    "print(dtlZoo(['dragonfly',0,0,1,0,1,0,1,0,0,1,0,0,6,0,0,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Section 5: Submission\n",
    "You will only be submitting your Jupyter Notebook file, *hwk2.ipynb*. Do not worry about submitting the additional files (*utils.py*, *restaurant.csv*, *zoo.csv*). Furthermore, as a reminder, part of your grade is your documentation. Each of the functions that you implemented as part of this assignment **must** be documented. Explain what the function is doing, its inputs/outputs, the logic behind your implementation, fancy Python ~~hacks~~ tricks, etc. Failure to include detailed documentation will result in a penalty.\n",
    "\n",
    "Please upload your *hwk2.ipynb* file to CMS by **Wednesday, 11/14 @ 6pm**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
